{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNge9i7dUKeWA46EADSoBrf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jannatkc/ADAtasks/blob/main/Merging_telecom_datadumps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1UIr1TkNt2r",
        "outputId": "9de79dad-c29f-47a1-9502-f01aca98ccac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZJn6caVMmQi",
        "outputId": "f309cd15-22b5-465c-ea25-0f7cc4f7f215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "root_directory = '/content/drive/MyDrive/Datadump_31stMar'\n",
        "\n",
        "os.chdir(root_directory)\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjOXGcrfN3UC",
        "outputId": "566e2470-cbbb-4fe1-8fd6-5586a785b4e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "airtel.xlsx\n",
            "bl.xlsx\n",
            "dataset_facebook-ads-library-scraper_2024-04-01_04-10-47-371.xlsx\n",
            "dataset_facebook-ads-library-scraper_2024-04-01_04-11-51-399.xlsx\n",
            "dataset_facebook-ads-library-scraper_2024-04-01_04-12-30-953.xlsx\n",
            "dataset_facebook-ads-library-scraper_2024-04-02_05-34-22-152.xlsx\n",
            "dataset_facebook-ads-library-scraper_2024-04-02_05-35-32-516.xlsx\n",
            "dataset_facebook-ads-library-scraper_2024-04-02_05-36-27-720.xlsx\n",
            "gp.xlsx\n",
            "newdata1.xlsx\n",
            "newdata.xlsx\n",
            "new_dump.xlsx\n",
            "robi.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read both Excel files\n",
        "df_atl = pd.read_excel('atl.xlsx')\n",
        "df_bl = pd.read_excel('bl.xlsx')\n",
        "df_gp = pd.read_excel('gp.xlsx')\n",
        "df_rb = pd.read_excel ('robi.xlsx')\n"
      ],
      "metadata": {
        "id": "CW6sa4t3N6x3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_name = 'pageName'\n",
        "\n",
        "# Get unique values in the specified column\n",
        "unique_values = df_rb[column_name].unique()\n",
        "\n",
        "# Print the unique values\n",
        "print(\"Unique values in column '{}':\".format(column_name))\n",
        "print(unique_values)"
      ],
      "metadata": {
        "id": "ZT7P4EKCOjdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'df_rb' is your DataFrame\n",
        "selected_columns = df_gp.iloc[:, 40:187].columns\n",
        "selected_columns\n"
      ],
      "metadata": {
        "id": "WjktIbtQRVN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'df' is your DataFrame\n",
        "common_columns_to_keep = [\n",
        "    'adArchiveID', 'startDate', 'endDate', 'pageName', 'publisherPlatform/0', 'publisherPlatform/1', 'publisherPlatform/2',\n",
        "       'publisherPlatform/3', 'snapshot/ad_creative_id', 'snapshot/body/markup/__html', 'snapshot/caption', 'snapshot/cards/0/body',\n",
        "    'snapshot/cta_text','snapshot/current_page_name', 'snapshot/display_format', 'snapshot/link_url', 'snapshot/images/0/original_image_url',\n",
        "       'snapshot/images/0/resized_image_url', 'snapshot/videos/0/video_hd_url',\n",
        "       'snapshot/videos/0/video_preview_image_url',\n",
        "       'snapshot/videos/0/video_sd_url','snapshot/title'\n",
        "]\n",
        "\n",
        "# Create a list of columns that exist in the DataFrame\n",
        "existing_columns = [col for col in common_columns_to_keep if col in df_atl.columns]\n",
        "\n",
        "# Update columns for df_atl\n",
        "df_atl1 = df_atl[existing_columns]\n",
        "\n",
        "# Create a list of columns that exist in the DataFrame\n",
        "existing_columns = [col for col in common_columns_to_keep if col in df_rb.columns]\n",
        "\n",
        "# Update columns for df_rb\n",
        "df_rb1 = df_rb[existing_columns]\n",
        "\n",
        "# Create a list of columns that exist in the DataFrame\n",
        "existing_columns = [col for col in common_columns_to_keep if col in df_gp.columns]\n",
        "\n",
        "# Update columns for df_gp\n",
        "df_gp1 = df_gp[existing_columns]\n",
        "\n",
        "# Create a list of columns that exist in the DataFrame\n",
        "existing_columns = [col for col in common_columns_to_keep if col in df_bl.columns]\n",
        "\n",
        "# Update columns for df_bl\n",
        "df_bl1 = df_bl[existing_columns]\n"
      ],
      "metadata": {
        "id": "lBrRTZavPz2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_atl1.head(5)\n",
        "df_rb1.head(5)\n",
        "df_bl1.head(5)\n",
        "df_gp1.head(5)"
      ],
      "metadata": {
        "id": "V5xOKDLmV-1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_atl1\n",
        "df_rb1\n",
        "df_bl1\n",
        "df_gp1"
      ],
      "metadata": {
        "id": "1MinaymWTLa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df_atl1, df_rb1, df_bl1, and df_gp1 have been defined\n",
        "\n",
        "# Define a list of all DataFrames\n",
        "dfs = [df_atl1, df_rb1, df_bl1, df_gp1]\n",
        "\n",
        "# Concatenate DataFrames along rows (axis=0)\n",
        "merged_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
        "\n",
        "# Replace NaN values with an empty string\n",
        "merged_df = merged_df.replace({pd.NA: ''})\n",
        "\n",
        "# Display the merged DataFrame\n",
        "merged_df\n"
      ],
      "metadata": {
        "id": "Lm0OmNn-U0BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'startDate' is the column containing Unix timestamps\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'startDate' is the column containing Unix timestamps\n",
        "df_timestamp = merged_df['startDate']\n",
        "merged_df['new_startDate'] = pd.to_datetime(df_timestamp, unit='s').dt.strftime('%m/%d/%Y')\n",
        "\n",
        "df_timestamp = merged_df['endDate']\n",
        "merged_df['new_endDate'] = pd.to_datetime(df_timestamp, unit='s').dt.strftime('%m/%d/%Y')\n",
        "\n",
        "# Add a new column for the upload date\n",
        "#merged_df['download_date'] = datetime.now().strftime('%m/%d/%Y')\n",
        "\n",
        "# Display the DataFrame with the new formatted_date and upload_date columns\n",
        "merged_df\n",
        "#Save tp excel\n",
        "merged_df.to_excel('newdump.xlsx', index=False)"
      ],
      "metadata": {
        "id": "OlKvPi9kZcKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merged_df['Format Start Date'] = pd.to_datetime(merged_df['Format Start Date'])\n",
        "#merged_df['Format End Date'] = pd.to_datetime(merged_df['Format End Date'])\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Assuming df is your DataFrame and the text is in the 'column_name' column\n",
        "# Replace 'column_name' with the actual name of your column\n",
        "\n",
        "# Define a function to delete text between patterns\n",
        "def delete_text_between_patterns(text):\n",
        "    # Replace text between <br> and </br> with an empty string\n",
        "    text = re.sub(r'<br />.*?<br />', '', text)\n",
        "\n",
        "    # Replace text between <a> and </a> with an empty string\n",
        "    text = re.sub(r' <a.*?</a>', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply the function to the specified column\n",
        "merged_df['snapshot/body/markup/__html'] = merged_df['snapshot/body/markup/__html'].apply(delete_text_between_patterns)\n",
        "\n",
        "\n",
        "merged_df['snapshot/body/markup/__html'] = merged_df['snapshot/body/markup/__html'].replace({'<br />': ''}, regex=True)\n",
        "\n",
        "merged_df['snapshot/body/markup/__html'] = merged_df['snapshot/body/markup/__html'].replace({'&quot;': ''}, regex=True)\n",
        "\n",
        "\n",
        "# Print the DataFrame with cleaned text\n",
        "merged_df.head(10)\n",
        "\n",
        "# Save the updated dataframe with the new 'Category' column to the same Excel file\n",
        "merged_df.to_excel('merged.xlsx', index=False)"
      ],
      "metadata": {
        "id": "wNtHvOmOvS8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Download date 4th Feb**\n"
      ],
      "metadata": {
        "id": "IlYOup5s554W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_excel ('dataset_facebook-ads-library-scraper_2024-02-04_05-29-34-335.xlsx')\n",
        "df2 = pd.read_excel ('dataset_facebook-ads-library-scraper_2024-02-04_05-30-03-925.xlsx')\n",
        "df3 = pd.read_excel ('dataset_facebook-ads-library-scraper_2024-02-04_05-30-30-946.xlsx')\n",
        "df4 = pd.read_excel ('dataset_facebook-ads-library-scraper_2024-02-04_05-32-09-892.xlsx')\n",
        "#Only keeping seleccting columns\n",
        "# Assuming 'df' is your DataFrame\n",
        "common_columns_to_keep = [\n",
        "    'adArchiveID', 'startDate', 'endDate', 'pageName', 'publisherPlatform/0', 'publisherPlatform/1', 'publisherPlatform/2',\n",
        "       'publisherPlatform/3', 'snapshot/ad_creative_id', 'snapshot/body/markup/__html', 'snapshot/caption', 'snapshot/cards/0/body',\n",
        "    'snapshot/cta_text', 'snapshot/display_format', 'snapshot/images/0/original_image_url', 'snapshot/videos/0/video_preview_image_url', 'snapshot/link_url', 'snapshot/title',\n",
        "]\n",
        "\n",
        "# Create a list of columns that exist in the DataFrame\n",
        "existing_columns = [col for col in common_columns_to_keep if col in df1.columns]\n",
        "\n",
        "# Update columns for df_atl\n",
        "df1 = df1[existing_columns]\n",
        "\n",
        "existing_columns1 = [col for col in common_columns_to_keep if col in df4.columns]\n",
        "\n",
        "# Update columns for df_rb\n",
        "df4 = df4[existing_columns1]\n",
        "\n",
        "# Update columns for df_gp\n",
        "df3 = df3[common_columns_to_keep]\n",
        "\n",
        "# Create a list of columns that exist in the DataFrame\n",
        "existing_columns2 = [col for col in common_columns_to_keep if col in df2.columns]\n",
        "\n",
        "# Update columns for df_bl\n",
        "df2 = df2[existing_columns2]\n",
        "##################################\n",
        "#Merging all the dfs\n",
        "# Define a list of all DataFrames\n",
        "dfs = [df1, df2, df3, df4]\n",
        "\n",
        "# Concatenate DataFrames along rows (axis=0)\n",
        "merged_df1 = pd.concat(dfs, axis=0, ignore_index=True)\n",
        "\n",
        "# Replace NaN values with an empty string\n",
        "merged_df1 = merged_df1.replace({pd.NA: ''})\n",
        "#########################################################################\n",
        "\n",
        "# convert Startdate and EndDate and add download date\n",
        "from datetime import datetime\n",
        "# Assuming 'startDate' is the column containing Unix timestamps\n",
        "df_timestamp = merged_df1['startDate']\n",
        "merged_df1['new_startDate'] = pd.to_datetime(df_timestamp, unit='s').dt.strftime('%m/%d/%y')\n",
        "\n",
        "df_timestamp = merged_df1['endDate']\n",
        "merged_df1['new_endDate'] = pd.to_datetime(df_timestamp, unit='s').dt.strftime('%m/%d/%y')\n",
        "\n",
        "# Add a new column for the upload date\n",
        "merged_df1['download_date'] = datetime.now().strftime('%m/%d/%y')\n",
        "\n",
        "#############################################################################\n",
        "#clean primary text\n",
        "import re\n",
        "\n",
        "# Assuming df is your DataFrame and the text is in the 'column_name' column\n",
        "# Replace 'column_name' with the actual name of your column\n",
        "\n",
        "# Define a function to delete text between patterns\n",
        "def delete_text_between_patterns(text):\n",
        "    # Replace text between <br> and </br> with an empty string\n",
        "    text = re.sub(r'<br />.*?<br />', '', text)\n",
        "\n",
        "    # Replace text between <a> and </a> with an empty string\n",
        "    text = re.sub(r' <a.*?</a>', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply the function to the specified column\n",
        "merged_df1['snapshot/body/markup/__html'] = merged_df1['snapshot/body/markup/__html'].apply(delete_text_between_patterns)\n",
        "\n",
        "\n",
        "merged_df1['snapshot/body/markup/__html'] = merged_df1['snapshot/body/markup/__html'].replace({'<br />': ''}, regex=True)\n",
        "\n",
        "merged_df1['snapshot/body/markup/__html'] = merged_df1['snapshot/body/markup/__html'].replace({'&quot;': ''}, regex=True)\n",
        "\n",
        "\n",
        "\n",
        "# Save the updated dataframe with the new 'Category' column to the same Excel file\n",
        "merged_df1.to_excel('merged1.xlsx', index=False)\n",
        "\n",
        "# Display the DataFrame with the new formatted_date and upload_date columns\n",
        "merged_df1\n"
      ],
      "metadata": {
        "id": "9ccA_Fxa4eT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing column names\n",
        "# Assuming 'df_subset' is your DataFrame with selected columns\n",
        "merged_df1.rename(columns={\n",
        "    'adArchiveID': 'Ad Archive ID',\n",
        "    'startDate': 'Start Date',\n",
        "    'endDate': 'End Date',\n",
        "    'pageName': 'Page name',\n",
        "    'publisherPlatform/0': 'Platform_0',\n",
        "    'publisherPlatform/1': 'Platform_1',\n",
        "    'publisherPlatform/2': 'Platform_2',\n",
        "    'publisherPlatform/3': 'Platform_3',\n",
        "    'snapshot/ad_creative_id': 'Creative identifier',\n",
        "    'snapshot/body/markup/__html': 'Primary Text (caption)',\n",
        "    'snapshot/caption': 'Preview URL (Button URL)',\n",
        "    'snapshot/cards/0/body': 'Primary Text V2',\n",
        "    'snapshot/cta_text': 'CTA TEXT',\n",
        "    'snapshot/display_format': 'Creative Format',\n",
        "    'snapshot/images/0/original_image_url': 'Original Image',\n",
        "    'snapshot/videos/0/video_preview_image_url': 'Video 1 preview',\n",
        "    'snapshot/link_url': 'Destination Url',\n",
        "    'snapshot/title': 'Headline',\n",
        "    # Add 'Video 1 HD' and 'Video 1 sd' if they are present in your DataFrame\n",
        "}, inplace=True)\n",
        "\n",
        "# Add 'Download Date' and 'Category' columns\n",
        "merged_df1['Download Date'] = datetime.now().strftime('%Y-%m-%d')\n",
        "merged_df1['Category'] = ''  # You can update this value as needed\n",
        "\n",
        "# Display the DataFrame with the new column names\n",
        "merged_df1"
      ],
      "metadata": {
        "id": "arlW9G855Ol3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'df_subset' is your DataFrame\n",
        "merged_df1['Preview Image'] = merged_df1['Original Image'] + merged_df1['Video 1 preview']\n",
        "# Save the updated dataframe with the new 'Category' column to the same Excel file\n",
        "merged_df1.to_excel('merged1.xlsx', index=False)\n",
        "# Display the DataFrame with the new column\n",
        "merged_df1"
      ],
      "metadata": {
        "id": "2P3XtbFAVi0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Download Date 5th feb**\n"
      ],
      "metadata": {
        "id": "zW8tsL5Uxrfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_excel ('5_2_at.xlsx')\n",
        "df2 = pd.read_excel ('5_2_bl.xlsx')\n",
        "df3 = pd.read_excel ('5_2_gp.xlsx')\n",
        "df4 = pd.read_excel ('5_2_robi.xlsx')\n",
        "\n",
        "#Only keeping seleccting columns\n",
        "# Assuming 'df' is your DataFrame\n",
        "common_columns_to_keep = [\n",
        "    'adArchiveID', 'startDate', 'endDate', 'pageName', 'publisherPlatform/0', 'publisherPlatform/1', 'publisherPlatform/2',\n",
        "       'publisherPlatform/3', 'snapshot/ad_creative_id', 'snapshot/body/markup/__html', 'snapshot/caption', 'snapshot/cards/0/body',\n",
        "    'snapshot/cta_text', 'snapshot/display_format', 'snapshot/images/0/original_image_url', 'snapshot/videos/0/video_preview_image_url', 'snapshot/link_url', 'snapshot/title',\n",
        "]\n",
        "\n",
        "# Create a list of columns that exist in the DataFrame\n",
        "existing_columns = [col for col in common_columns_to_keep if col in df1.columns]\n",
        "\n",
        "# Update columns for df_atl\n",
        "df1 = df1[existing_columns]\n",
        "\n",
        "existing_columns1 = [col for col in common_columns_to_keep if col in df4.columns]\n",
        "\n",
        "# Update columns for df_rb\n",
        "df4 = df4[existing_columns1]\n",
        "\n",
        "# Update columns for df_gp\n",
        "df3 = df3[common_columns_to_keep]\n",
        "\n",
        "# Create a list of columns that exist in the DataFrame\n",
        "existing_columns2 = [col for col in common_columns_to_keep if col in df2.columns]\n",
        "\n",
        "# Update columns for df_bl\n",
        "df2 = df2[existing_columns2]\n",
        "##################################\n",
        "#Merging all the dfs\n",
        "# Define a list of all DataFrames\n",
        "dfs = [df1, df2, df3, df4]\n",
        "\n",
        "# Concatenate DataFrames along rows (axis=0)\n",
        "merged_df1 = pd.concat(dfs, axis=0, ignore_index=True)\n",
        "\n",
        "# Replace NaN values with an empty string\n",
        "merged_df1 = merged_df1.replace({pd.NA: ''})\n",
        "#########################################################################\n",
        "\n",
        "# convert Startdate and EndDate and add download date\n",
        "from datetime import datetime\n",
        "# Assuming 'startDate' is the column containing Unix timestamps\n",
        "df_timestamp = merged_df1['startDate']\n",
        "merged_df1['Format Start Date'] = pd.to_datetime(df_timestamp, unit='s').dt.strftime('%m/%d/%y')\n",
        "\n",
        "df_timestamp = merged_df1['endDate']\n",
        "merged_df1['Format End Date'] = pd.to_datetime(df_timestamp, unit='s').dt.strftime('%m/%d/%y')\n",
        "\n",
        "# Add a new column for the upload date\n",
        "merged_df1['Downloadd Date'] = datetime.now().strftime('%m/%d/%y')\n",
        "\n",
        "#############################################################################\n",
        "#clean primary text\n",
        "import re\n",
        "\n",
        "# Assuming df is your DataFrame and the text is in the 'column_name' column\n",
        "# Replace 'column_name' with the actual name of your column\n",
        "\n",
        "# Define a function to delete text between patterns\n",
        "def delete_text_between_patterns(text):\n",
        "    # Replace text between <br> and </br> with an empty string\n",
        "    text = re.sub(r'<br />.*?<br />', '', text)\n",
        "\n",
        "    # Replace text between <a> and </a> with an empty string\n",
        "    text = re.sub(r' <a.*?</a>', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply the function to the specified column\n",
        "merged_df1['snapshot/body/markup/__html'] = merged_df1['snapshot/body/markup/__html'].apply(delete_text_between_patterns)\n",
        "\n",
        "\n",
        "merged_df1['snapshot/body/markup/__html'] = merged_df1['snapshot/body/markup/__html'].replace({'<br />': ''}, regex=True)\n",
        "\n",
        "merged_df1['snapshot/body/markup/__html'] = merged_df1['snapshot/body/markup/__html'].replace({'&quot;': ''}, regex=True)\n",
        "\n",
        "\n",
        "\n",
        "# Save the updated dataframe with the new 'Category' column to the same Excel file\n",
        "merged_df1.to_excel('merged2.xlsx', index=False)\n",
        "\n",
        "# Display the DataFrame with the new formatted_date and upload_date columns\n",
        "merged_df1\n"
      ],
      "metadata": {
        "id": "tEz42V-ixpuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Download date 6th feb**"
      ],
      "metadata": {
        "id": "pbDQLkUffXHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df1 = pd.read_excel ('dataset_facebook-ads-library-scraper_2024-02-06_03-27-27-486.xlsx')\n",
        "df2 = pd.read_excel ('dataset_facebook-ads-library-scraper_2024-02-06_03-29-15-132.xlsx')\n",
        "df3 = pd.read_excel ('dataset_facebook-ads-library-scraper_2024-02-06_03-30-05-437.xlsx')\n",
        "df4 = pd.read_excel ('dataset_facebook-ads-library-scraper_2024-02-06_03-32-16-182.xlsx')\n",
        "#Only keeping seleccting columns\n",
        "# Assuming 'df' is your DataFrame\n",
        "common_columns_to_keep = [\n",
        "    'adArchiveID', 'startDate', 'endDate', 'pageName', 'publisherPlatform/0', 'publisherPlatform/1', 'publisherPlatform/2',\n",
        "       'publisherPlatform/3', 'snapshot/ad_creative_id', 'snapshot/body/markup/__html', 'snapshot/caption', 'snapshot/cards/0/body',\n",
        "    'snapshot/cta_text', 'snapshot/display_format', 'snapshot/images/0/original_image_url', 'snapshot/videos/0/video_preview_image_url', 'snapshot/link_url', 'snapshot/title',\n",
        "]\n",
        "\n",
        "# Create a list of columns that exist in the DataFrame\n",
        "existing_columns = [col for col in common_columns_to_keep if col in df1.columns]\n",
        "\n",
        "# Update columns for df_atl\n",
        "df1 = df1[existing_columns]\n",
        "\n",
        "existing_columns1 = [col for col in common_columns_to_keep if col in df4.columns]\n",
        "\n",
        "# Update columns for df_rb\n",
        "df4 = df4[existing_columns1]\n",
        "\n",
        "# Update columns for df_gp\n",
        "df3 = df3[common_columns_to_keep]\n",
        "\n",
        "# Create a list of columns that exist in the DataFrame\n",
        "existing_columns2 = [col for col in common_columns_to_keep if col in df2.columns]\n",
        "\n",
        "# Update columns for df_bl\n",
        "df2 = df2[existing_columns2]\n",
        "##################################\n",
        "#Merging all the dfs\n",
        "# Define a list of all DataFrames\n",
        "dfs = [df1, df2, df3, df4]\n",
        "\n",
        "# Concatenate DataFrames along rows (axis=0)\n",
        "merged_df1 = pd.concat(dfs, axis=0, ignore_index=True)\n",
        "\n",
        "# Replace NaN values with an empty string\n",
        "merged_df1 = merged_df1.replace({pd.NA: ''})\n",
        "#########################################################################\n",
        "\n",
        "# convert Startdate and EndDate and add download date\n",
        "from datetime import datetime\n",
        "# Assuming 'startDate' is the column containing Unix timestamps\n",
        "df_timestamp = merged_df1['startDate']\n",
        "merged_df1['new_startDate'] = pd.to_datetime(df_timestamp, unit='s').dt.strftime('%m/%d/%y')\n",
        "\n",
        "df_timestamp = merged_df1['endDate']\n",
        "merged_df1['new_endDate'] = pd.to_datetime(df_timestamp, unit='s').dt.strftime('%m/%d/%y')\n",
        "\n",
        "# Add a new column for the upload date\n",
        "merged_df1['download_date'] = datetime.now().strftime('%m/%d/%y')\n",
        "\n",
        "#############################################################################\n",
        "\n",
        "\n",
        "# Save the updated dataframe with the new 'Category' column to the same Excel file\n",
        "merged_df1.to_excel('new_dump.xlsx', index=False)\n",
        "\n",
        "# Display the DataFrame with the new formatted_date and upload_date columns\n",
        "merged_df1\n"
      ],
      "metadata": {
        "id": "zKACJPSSX79-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df1 = pd.read_excel('dataset_facebook-ads-library-scraper_2024-04-02_05-34-22-152.xlsx')\n",
        "df2 = pd.read_excel('dataset_facebook-ads-library-scraper_2024-04-02_05-35-32-516.xlsx')\n",
        "df3 = pd.read_excel('dataset_facebook-ads-library-scraper_2024-04-02_05-36-27-720.xlsx')\n",
        "\n",
        "#Only keeping seleccting columns\n",
        "# Assuming 'df' is your DataFrame\n",
        "common_columns_to_keep = [\n",
        "    'adArchiveID', 'startDate', 'endDate', 'pageName', 'publisherPlatform/0', 'publisherPlatform/1', 'publisherPlatform/2',\n",
        "       'publisherPlatform/3', 'snapshot/ad_creative_id', 'snapshot/body/markup/__html', 'snapshot/caption', 'snapshot/cards/0/body',\n",
        "    'snapshot/cta_text', 'snapshot/display_format', 'snapshot/images/0/original_image_url','snapshot/images/0/resized_image_url',\n",
        "    'snapshot/link_url', 'snapshot/title', 'snapshot/videos/0/video_hd_url','snapshot/videos/0/video_preview_image_url',\n",
        "    'snapshot/videos/0/video_sd_url'\n",
        "]\n",
        "\n",
        "# Create a list of columns that exist in the DataFrame\n",
        "existing_columns = [col for col in common_columns_to_keep if col in df1.columns]\n",
        "\n",
        "# Update columns for df_atl\n",
        "df1 = df1[existing_columns]\n",
        "\n",
        "#existing_columns1 = [col for col in common_columns_to_keep if col in df4.columns]\n",
        "\n",
        "# Update columns for df_rb\n",
        "#df4 = df4[existing_columns1]\n",
        "\n",
        "#  Create a list of columns that exist in the DataFrame\n",
        "existing_columns3 = [col for col in common_columns_to_keep if col in df3.columns]\n",
        "\n",
        "# Update columns for df_gp\n",
        "df3 = df3[existing_columns3]\n",
        "\n",
        "# Create a list of columns that exist in the DataFrame\n",
        "existing_columns2 = [col for col in common_columns_to_keep if col in df2.columns]\n",
        "\n",
        "# Update columns for df_bl\n",
        "df2 = df2[existing_columns2]\n",
        "##################################\n",
        "#Merging all the dfs\n",
        "# Define a list of all DataFrames\n",
        "dfs = [df1, df2, df3]\n",
        "\n",
        "# Concatenate DataFrames along rows (axis=0)\n",
        "merged_df1 = pd.concat(dfs, axis=0, ignore_index=True)\n",
        "\n",
        "# Replace NaN values with an empty string\n",
        "merged_df1 = merged_df1.replace({pd.NA: ''})\n",
        "#########################################################################\n",
        "\n",
        "# convert Startdate and EndDate and add download date\n",
        "from datetime import datetime\n",
        "# Assuming 'startDate' is the column containing Unix timestamps\n",
        "df_timestamp = merged_df1['startDate']\n",
        "merged_df1['new_startDate'] = pd.to_datetime(df_timestamp, unit='s').dt.strftime('%m/%d/%y')\n",
        "\n",
        "df_timestamp = merged_df1['endDate']\n",
        "merged_df1['new_endDate'] = pd.to_datetime(df_timestamp, unit='s').dt.strftime('%m/%d/%y')\n",
        "\n",
        "# Add a new column for the upload date\n",
        "merged_df1['download_date'] = datetime.now().strftime('%m/%d/%y')\n",
        "\n",
        "#############################################################################\n",
        "\n",
        "\n",
        "# Save the updated dataframe with the new 'Category' column to the same Excel file\n",
        "merged_df1.to_excel('newdata1.xlsx', index=False)\n",
        "\n",
        "# Display the DataFrame with the new formatted_date and upload_date columns\n",
        "merged_df1\n"
      ],
      "metadata": {
        "id": "4lG-gAYVfikL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pYrmDuXEGzkb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}